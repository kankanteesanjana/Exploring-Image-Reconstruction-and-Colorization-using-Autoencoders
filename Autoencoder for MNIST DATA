# import the necessary packages
import os
import torch
# set device to 'cpu' or 'cuda' (GPU) based on availability
# for model training and testing
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"
# define model hyperparameters
LR = 0.001
PATIENCE = 2
IMAGE_SIZE = 32
CHANNELS = 1
BATCH_SIZE = 64
EMBEDDING_DIM = 2 # 2D latent space
EPOCHS = 10
# create output directory
output_dir = "./output_mnist"
os.makedirs(output_dir, exist_ok=True)
# create the training_progress directory inside the output directory
training_progress_dir = os.path.join(output_dir, "training_progress")
os.makedirs(training_progress_dir, exist_ok=True)
# create the model_weights directory inside the output directory
# for storing autoencoder weights
model_weights_dir = os.path.join(output_dir, "model_weights")
os.makedirs(model_weights_dir, exist_ok=True)
# define model_weights, reconstruction & real before training images path
MODEL_WEIGHTS_PATH = os.path.join(model_weights_dir, "best_autoencoder.pt")
FILE_RECON_BEFORE_TRAINING = os.path.join(output_dir, "reconstruct_before_train.png")
FILE_REAL_BEFORE_TRAINING = os.path.join(
    output_dir, "real_test_images_before_train.png"
)
# define reconstruction & real after training images path
FILE_RECON_AFTER_TRAINING = os.path.join(output_dir, "reconstruct_after_train.png")
FILE_REAL_AFTER_TRAINING = os.path.join(output_dir, "real_test_images_after_train.png")
# define latent space and image grid embeddings plot path
LATENT_SPACE_PLOT = os.path.join(output_dir, "embedding_visualize.png")
IMAGE_GRID_EMBEDDINGS_PLOT = os.path.join(output_dir, "image_grid_on_embeddings.png")
# define class labels dictionary for Fashion MNIST dataset
# CLASS_LABELS = {
#     0: "T-shirt/top",
#     1: "Trouser",
#     2: "Pullover",
#     3: "Dress",
#     4: "Coat",
#     5: "Sandal",
#     6: "Shirt",
#     7: "Sneaker",
#     8: "Bag",
#     9: "Ankle boot",
# }
# define class labels dictionary for MNIST dataset
CLASS_LABELS = {
    0: "0",
    1: "1",
    2: "2",
    3: "3",
    4: "4",
    5: "5",
    6: "6",
    7: "7",
    8: "8",
    9: "9",
}
# import the necessary packages
import matplotlib
import numpy as np
import torch
import torchvision
# from pyimagesearch import config
matplotlib.use("agg")
import matplotlib.cm as cm
import matplotlib.colors as mcolors
import matplotlib.pyplot as plt
from matplotlib.offsetbox import AnnotationBbox, OffsetImage
from tqdm import tqdm
def extract_random_images(data_loader, num_images):
    # initialize empty lists to store all images and labels
    all_images = []
    all_labels = []
    # iterate through the data loader to get images and labels
    for images, labels in data_loader:
        # append the current batch of images and labels to the respective lists
        all_images.append(images)
        all_labels.append(labels)
        # stop the iteration if the total number of images exceeds 1000
        if len(all_images) * data_loader.batch_size > 1000:
            break
    # concatenate all the images and labels tensors along the 0th dimension
    all_images = torch.cat(all_images, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    # generate random indices for selecting a subset of images and labels
    random_indices = np.random.choice(len(all_images), num_images, replace=False)
    # use the random indices to extract the corresponding images and labels
    random_images = all_images[random_indices]
    random_labels = all_labels[random_indices]
    # return the randomly selected images and labels to the calling function
    return random_images, random_labels
def display_images(images, labels, num_images_per_row, title, filename=None, show=True):
    # calculate the number of rows needed to display all the images
    num_rows = len(images) // num_images_per_row
    # create a grid of images using torchvision's make_grid function
    grid = torchvision.utils.make_grid(
        images.cpu(), nrow=num_images_per_row, padding=2, normalize=True
    )
    # convert the grid to a NumPy array and transpose it to
    # the correct dimensions
    grid_np = grid.numpy().transpose((1, 2, 0))
    # create a new figure with the appropriate size
    plt.figure(figsize=(num_images_per_row * 2, num_rows * 2))
    # show the grid of images
    plt.imshow(grid_np)
    # remove the axis ticks
    plt.axis("off")
    # set the title of the plot
    plt.title(title, fontsize=16)
     # add labels for each image in the grid
    for i in range(len(images)):
        # calculate the row and column of the current image in the grid
        row = i // num_images_per_row
        col = i % num_images_per_row
        # get the name of the label for the current image
        label_name = CLASS_LABELS[labels[i].item()]
        # add the label name as text to the plot
        plt.text(
            col * (images.shape[3] + 2) + images.shape[3] // 2,
            (row + 1) * (images.shape[2] + 2) - 5,
            label_name,
            fontsize=12,
            ha="center",
            va="center",
            color="white",
            bbox=dict(facecolor="black", alpha=0.5, lw=0),
        )
    # if show is True, display the plot
    if show:
        plt.show()
    else:
        # otherwise, save the plot to a file and close the figure
        plt.savefig(filename, bbox_inches="tight")
        plt.close()
def display_random_images(
    data_loader,
    encoder=None,
    decoder=None,
    file_recon=None,
    file_real=None,
    title_recon=None,
    title_real=None,
    display_real=True,
    num_images=32,
    num_images_per_row=8,
):
    # extract a random subset of images and labels from the data loader
    random_images, random_labels = extract_random_images(data_loader, num_images)
    # if an encoder and decoder are provided,
    # use them to generate reconstructions
    if encoder is not None and decoder is not None:
        # set the encoder and decoder to evaluation mode
        encoder.eval()
        decoder.eval()
        # move the random images to the appropriate device
        random_images = random_images.to(DEVICE)
        # generate embeddings for the random images using the encoder
        random_embeddings = encoder(random_images)
        # generate reconstructions for the random images using the decoder
        random_reconstructions = decoder(random_embeddings)
        # display the reconstructed images
        display_images(
            random_reconstructions.cpu(),
            random_labels,
            num_images_per_row,
            title_recon,
            file_recon,
            show=False,
        )
        # if specified, also display the original images
        if display_real:
            display_images(
                random_images.cpu(),
                random_labels,
                num_images_per_row,
                title_real,
                file_real,
                show=False,
            )
    # if no encoder and decoder are provided, simply display the original images
    else:
        display_images(
            random_images, random_labels, num_images_per_row, title="Real Images"
        )
def validate(encoder, decoder, test_loader, criterion):
    # set the encoder and decoder to evaluation mode
    encoder.eval()
    decoder.eval()
    # initialize the running loss to 0.0
    running_loss = 0.0
    # disable gradient calculation during validation
    with torch.no_grad():
        # iterate through the test loader
        for batch_idx, (data, _) in tqdm(
            enumerate(test_loader), total=len(test_loader)
        ):
            # move the data to the appropriate device CPU/GPU
            data = data.to(DEVICE)
            # encode the data using the encoder
            encoded = encoder(data)
            # decode the encoded data using the decoder
            decoded = decoder(encoded)
            # calculate the loss between the decoded and original data
            loss = criterion(decoded, data)
            # add the loss to the running loss
            running_loss += loss.item()
    # calculate the average loss over all batches
    # and return to the calling function
    return running_loss / len(test_loader)
def get_test_embeddings(test_loader, encoder):
    # switch the model to evaluation mode
    encoder.eval()
    # initialize empty lists to store the embeddings and labels
    points = []
    label_idcs = []
    # iterate through the test loader
    for i, data in enumerate(test_loader):
        # move the images and labels to the appropriate device
        img, label = [d.to(DEVICE) for d in data]
        # encode the test images using the encoder
        proj = encoder(img)
        # convert the embeddings and labels to NumPy arrays
        # and append them to the respective lists
        points.extend(proj.detach().cpu().numpy())
        label_idcs.extend(label.detach().cpu().numpy())
        # free up memory by deleting the images and labels
        del img, label
    # convert the embeddings and labels to NumPy arrays
    points = np.array(points)
    label_idcs = np.array(label_idcs)
    # return the embeddings and labels to the calling function
    return points, label_idcs
def plot_latent_space(test_loader, encoder, show=False):
    # get the embeddings and labels for the test images
    points, label_idcs = get_test_embeddings(test_loader, encoder)
    # create a new figure and axis for the plot
    fig, ax = plt.subplots(figsize=(10, 10) if not show else (8, 8))
    # create a scatter plot of the embeddings, colored by the labels
    scatter = ax.scatter(
        x=points[:, 0],
        y=points[:, 1],
        s=2.0,
        c=label_idcs,
        cmap="tab10",
        alpha=0.9,
        zorder=2,
    )
    # remove the top and right spines from the plot
    ax.spines["right"].set_visible(False)
    ax.spines["top"].set_visible(False)
    # add a colorbar to the plot
    cbar = plt.colorbar(scatter, ax=ax)
    cbar.ax.set_ylabel("Labels", rotation=270, labelpad=20)
    # if show is True, display the plot
    if show:
        # add a grid to the plot
        ax.grid(True, color="lightgray", alpha=1.0, zorder=0)
        plt.show()
    # otherwise, save the plot to a file and close the figure
    else:
        plt.savefig(LATENT_SPACE_PLOT, bbox_inches="tight")
        plt.close()
def get_random_test_images_embeddings(test_loader, encoder, imgs_visualize=5000):
    # get all the images and labels from the test loader
    all_images, all_labels = [], []
    for batch in test_loader:
        images_batch, labels_batch = batch
        all_images.append(images_batch)
        all_labels.append(labels_batch)
    # concatenate all the images and labels into a single tensor
    all_images = torch.cat(all_images, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    # randomly select a subset of the images and labels to visualize
    index = np.random.choice(range(len(all_images)), imgs_visualize)
    images = all_images[index]
    labels = all_labels[index]
    # get the embeddings for all the test images
    points, _ = get_test_embeddings(test_loader, encoder)
    # select the embeddings corresponding to the randomly selected images
    embeddings = points[index]
    # return the randomly selected images, their labels, and their embeddings
    return images, labels, embeddings
def plot_image_grid_on_embeddings(
    test_loader, encoder, decoder, grid_size=15, figsize=12, show=True
):
    # get a random subset of test images
    # and their corresponding embeddings and labels
    _, labels, embeddings = get_random_test_images_embeddings(test_loader, encoder)
    # create a single figure for the plot
    fig, ax = plt.subplots(figsize=(figsize, figsize))
    # define a custom color map with discrete colors for each unique label
    unique_labels = np.unique(labels)
    num_classes = len(unique_labels)
    cmap = matplotlib.colormaps["rainbow"]#cm.get_cmap(, num_classes)
    bounds = np.linspace(0, num_classes, num_classes + 1)
    norm = mcolors.BoundaryNorm(bounds, cmap.N)
    # Plot the scatter plot of the embeddings colored by label
    scatter = ax.scatter(
        embeddings[:, 0],
        embeddings[:, 1],
        cmap=cmap,
        c=labels,
        norm=norm,
        alpha=0.8,
        s=300,
    )
    # Create the colorbar with discrete ticks corresponding to unique labels
    cb = plt.colorbar(scatter, ticks=range(num_classes), spacing="proportional", ax=ax)
    cb.set_ticklabels(unique_labels)
    # Create the grid of images to overlay on the scatter plot
    x = np.linspace(embeddings[:, 0].min(), embeddings[:, 0].max(), grid_size)
    y = np.linspace(embeddings[:, 1].max(), embeddings[:, 1].min(), grid_size)
    xv, yv = np.meshgrid(x, y)
    grid = np.column_stack((xv.ravel(), yv.ravel()))
    # convert the numpy array to a PyTorch tensor
    # and get reconstructions from the decoder
    grid_tensor = torch.tensor(grid, dtype=torch.float32)
    reconstructions = decoder(grid_tensor.to(DEVICE))
    # overlay the images on the scatter plot
    for i, (grid_point, img) in enumerate(zip(grid, reconstructions)):
        img = img.squeeze().detach().cpu().numpy()
        imagebox = OffsetImage(img, cmap="Greys", zoom=0.5)
        ab = AnnotationBbox(
            imagebox, grid_point, frameon=False, pad=0.0, box_alignment=(0.5, 0.5)
        )
        ax.add_artist(ab)
  
    if show:
        # add a grid to the plot
        ax.grid(True, color="lightgray", alpha=1.0, zorder=0)
        plt.show()
    # otherwise, save the plot to a file and close the figure
    else:
        plt.savefig(IMAGE_GRID_EMBEDDINGS_PLOT, bbox_inches="tight")
        plt.close()
# import the necessary packages
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
class Encoder(nn.Module):
    def __init__(self, image_size, channels, embedding_dim):
        super(Encoder, self).__init__()
        # define convolutional layers
        self.conv1 = nn.Conv2d(channels, 32, kernel_size=3, stride=2, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)
        # variable to store the shape of the output tensor before flattening
        # the features, it will be used in decoders input while reconstructing
        self.shape_before_flattening = None
        # compute the flattened size after convolutions
        flattened_size = (image_size // 8) * (image_size // 8) * 128
        # define fully connected layer to create embeddings
        self.fc = nn.Linear(flattened_size, embedding_dim)
    def forward(self, x):
        # apply ReLU activations after each convolutional layer
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        # store the shape before flattening
        self.shape_before_flattening = x.shape[1:]
        # flatten the tensor
        x = x.view(x.size(0), -1)
        # apply fully connected layer to generate embeddings
        x = self.fc(x)
        return x
class Decoder(nn.Module):
    def __init__(self, embedding_dim, shape_before_flattening, channels):
        super(Decoder, self).__init__()
        # define fully connected layer to unflatten the embeddings
        self.fc = nn.Linear(embedding_dim, np.prod(shape_before_flattening))
        # store the shape before flattening
        self.reshape_dim = shape_before_flattening
        # define transpose convolutional layers
        self.deconv1 = nn.ConvTranspose2d(
            128, 128, kernel_size=3, stride=2, padding=1, output_padding=1
        )
        self.deconv2 = nn.ConvTranspose2d(
            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1
        )
        self.deconv3 = nn.ConvTranspose2d(
            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1
        )
        # define final convolutional layer to generate output image
        self.conv1 = nn.Conv2d(32, channels, kernel_size=3, stride=1, padding=1)
    def forward(self, x):
        # apply fully connected layer to unflatten the embeddings
        x = self.fc(x)
        # reshape the tensor to match shape before flattening
        x = x.view(x.size(0), *self.reshape_dim)
        # apply ReLU activations after each transpose convolutional layer
        x = F.relu(self.deconv1(x))
        x = F.relu(self.deconv2(x))
        x = F.relu(self.deconv3(x))
        # apply sigmoid activation to the final convolutional layer to generate output image
        x = torch.sigmoid(self.conv1(x))
        return x
# USAGE
# python train.py
# import the necessary packages
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from tqdm.auto import tqdm
# from pyimagesearch import config, network, utils
# define the transformation to be applied to the data
transform = transforms.Compose([transforms.Pad(padding=2), transforms.ToTensor()])
# load the FashionMNIST training data and create a dataloader
# trainset = datasets.FashionMNIST("data", train=True, download=True, transform=transform)
trainset = datasets.MNIST("data", train=True, download=True, transform=transform)

train_loader = torch.utils.data.DataLoader(
    trainset, batch_size=BATCH_SIZE, shuffle=True
)
# Load the FashionMNIST test data and create a dataloader
# testset = datasets.FashionMNIST("data", train=False, download=True, transform=transform)
testset = datasets.MNIST("data", train=False, download=True, transform=transform)

test_loader = torch.utils.data.DataLoader(
    testset, batch_size=BATCH_SIZE, shuffle=True
)
# create an encoder instance with the specified channels,
# image size, and embedding dimensions
# then move it to the device (CPU or GPU) specified in the config
encoder = Encoder(
    channels=CHANNELS,
    image_size=IMAGE_SIZE,
    embedding_dim=EMBEDDING_DIM,
).to(DEVICE)
# pass the dummy input through the encoder and
# get the output (encoded representation)
dummy_input = torch.randn(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)
enc_out = encoder(dummy_input.to(DEVICE))
# get the shape of the tensor before it was flattened in the encoder
shape_before_flattening = encoder.shape_before_flattening
# create a decoder instance with the specified embedding dimensions,
# shape before flattening, and channels
# then move it to the device (CPU or GPU) specified in the config
decoder = Decoder(EMBEDDING_DIM, shape_before_flattening, CHANNELS).to(DEVICE)
# instantiate loss, optimizer, and scheduler
criterion = nn.MSELoss()#nn.BCELoss()
optimizer = optim.Adam(
    list(encoder.parameters()) + list(decoder.parameters()), lr=LR
)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode="min", factor=0.1, patience=PATIENCE, verbose=True
)
# call the 'display_random_images' function from the 'utils' module to display
# and save random reconstructed images from the test data
# before the autoencoder training
display_random_images(
    test_loader,
    encoder,
    decoder,
    title_recon="Reconstructed Before Training",
    title_real="Real Test Images",
    file_recon=FILE_RECON_BEFORE_TRAINING,
    file_real=FILE_REAL_BEFORE_TRAINING,
)
# initialize the best validation loss as infinity
best_val_loss = float("inf")
# start training by looping over the number of epochs
for epoch in range(EPOCHS):
    print(f"Epoch: {epoch + 1}/{EPOCHS}")
    # set the encoder and decoder models to training mode
    encoder.train()
    decoder.train()
    # initialize running loss as 0
    running_loss = 0.0
    # loop over the batches of the training dataset
    for batch_idx, (data, _) in tqdm(enumerate(train_loader), total=len(train_loader)):
        # move the data to the device (GPU or CPU)
        data = data.to(DEVICE)
        # reset the gradients of the optimizer
        optimizer.zero_grad()
        # forward pass: encode the data and decode the encoded representation
        encoded = encoder(data)
        decoded = decoder(encoded)
        # compute the reconstruction loss between the decoded output and
        # the original data
        loss = criterion(decoded, data)
        # backward pass: compute the gradients
        loss.backward()
        # update the model weights
        optimizer.step()
        # accumulate the loss for the current batch
        running_loss += loss.item()
        # compute the average training loss for the epoch
    train_loss = running_loss / len(train_loader)
    # compute the validation loss
    val_loss = validate(encoder, decoder, test_loader, criterion)
    # print training and validation loss for current epoch
    print(
        f"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} "
        f"| Val Loss: {val_loss:.4f}"
    )
    # save best model weights based on validation loss
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        torch.save(
            {"encoder": encoder.state_dict(), "decoder": decoder.state_dict()},
            MODEL_WEIGHTS_PATH,
        )
    # adjust learning rate based on the validation loss
    scheduler.step(val_loss)
    # save validation output reconstruction for the current epoch
    display_random_images(
        data_loader=test_loader,
        encoder=encoder,
        decoder=decoder,
        file_recon=os.path.join(
            training_progress_dir, f"epoch{epoch + 1}_test_recon.png"
        ),
        display_real=False,
    )
print("Training finished!")
# USAGE
# import the necessary packages
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from tqdm.auto import tqdm

# generate a random input tensor with the same shape as the input images
# (1: batch size, config.CHANNELS: number of channels,
# config.IMAGE_SIZE: height, config.IMAGE_SIZE: width)
dummy_input = torch.randn(1, CHANNELS, IMAGE_SIZE, IMAGE_SIZE)
# create an encoder instance with the specified channels,
# image size, and embedding dimensions
# then move it to the device (CPU or GPU) specified in the config
encoder = Encoder(
    channels=CHANNELS,
    image_size=IMAGE_SIZE,
    embedding_dim=EMBEDDING_DIM,
).to(DEVICE)
# pass the dummy input through the encoder and
# get the output (encoded representation)
enc_out = encoder(dummy_input.to(DEVICE))
# get the shape of the tensor before it was flattened in the encoder
shape_before_flattening = encoder.shape_before_flattening
# create a decoder instance with the specified embedding dimensions,
# shape before flattening, and channels
# then move it to the device (CPU or GPU) specified in the config
decoder = Decoder(EMBEDDING_DIM, shape_before_flattening, CHANNELS).to(DEVICE)
# load the saved state dictionaries for the encoder and decoder
checkpoint = torch.load(MODEL_WEIGHTS_PATH)
encoder.load_state_dict(checkpoint["encoder"])
decoder.load_state_dict(checkpoint["decoder"])
# set the models to evaluation mode
encoder = encoder.eval()
decoder = decoder.eval()
# define the transformation to be applied to the data
transform = transforms.Compose([transforms.Pad(padding=2), transforms.ToTensor()])
# load the test data
# testset = datasets.FashionMNIST("data", train=False, download=True, transform=transform)
testset = datasets.MNIST("data", train=False, download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(
    testset, batch_size=BATCH_SIZE, shuffle=True
)
# call the 'display_random_images' function from the 'utils' module to display
# and save random reconstructed images from the test data
# after the autoencoder training
display_random_images(
    test_loader,
    encoder,
    decoder,
    title_recon="Reconstructed After Training",
    title_real="Real Test Images After Training",
    file_recon=FILE_RECON_AFTER_TRAINING,
    file_real=FILE_REAL_AFTER_TRAINING,
)
plot_latent_space(test_loader, encoder, show=False)
plot_image_grid_on_embeddings(test_loader, encoder, decoder, show=False)

